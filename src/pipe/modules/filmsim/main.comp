#version 460
#extension GL_GOOGLE_include_directive    : enable
#extension GL_EXT_nonuniform_qualifier    : enable
#include "shared.glsl"
#include "../rt/colour.glsl"
layout(local_size_x = DT_LOCAL_SIZE_X, local_size_y = DT_LOCAL_SIZE_Y, local_size_z = 1) in;
layout(std140, set = 0, binding = 0) uniform global_t
{
  int frame;
  int frame_cnt;
} global;
layout(std140, set = 0, binding = 1) uniform params_t
{
  int   film;
  float ev_film;
  float gamma_film;
  int   paper;
  float ev_paper;
  float gamma_paper;
  int   grain;
  float grain_size;
  float grain_uniformity;
  int   enlarger;
  float filter_c;
  float filter_m;
  float filter_y;
  float tune_m;
  float tune_y;
} params;
layout(set = 1, binding = 0) uniform sampler2D img_in;
layout(set = 1, binding = 1) uniform writeonly image2D img_out;
layout(set = 1, binding = 2) uniform sampler2D img_filmsim;
layout(set = 1, binding = 3) uniform sampler2D img_coeff;   // spectral upsampling for emission
#include "shared/upsample.glsl"

// TODO this needs a direct simulation method, is too slow for large n
int poisson(inout uint seed, float lambda)
{
  float u = 1.0;
  float b = exp(-lambda);
  int m = 400; // should be > 2x lambda or else you can see the bias
  for(int i=0;i<m;i++)
  {
    if(u < b) return i;
    u *= mrand(seed);
  }
  return m; // sorry, here.
}

int binom(inout uint seed, int n, float p)
{
#if 1 // gaussian approximation, good for large n*p
  float u = n*p;
  float s = sqrt(n*p*(1.0-p));
  vec2 r = vec2(mrand(seed), mrand(seed));
  return max(0, int(u + s * warp_gaussian(r).x));
#else // bernoulli trials
  int k = 0;
  for(int i=0;i<n;i++)
    if(mrand(seed) < p) k++;
  return k;
#endif
}

float envelope(float w)
{
  return smoothstep(380.0, 400.0, w)*(1.0-smoothstep(700.0, 730.0, w));
}

vec3 thorlabs_filters(float w)
{
  // this is what looks like it might be a good fit to the thorlabs filters in a plot.
  // the lines marked with XXX smooth out the yellow/magenta transition at 500nm such that the two
  // filters sum to one. as a result i can fit the neutral filter configuration for supra and portra
  // without using negative weights (!!).
  // makes me think that somewhere else in the pipeline there might be something off.
  float cyan    = 0.93*smoothstep(345.0, 380.0, w)*(1.0-smoothstep(545.0, 590.0, w))+0.7*smoothstep(775.0,810,w);
  // float magenta = 0.9*smoothstep(355.0, 380.0, w)*(1.0-smoothstep(475.0, 505.0, w));
  float magenta = 0.9*smoothstep(355.0, 380.0, w)*(1.0-smoothstep(475.0, 525.0, w)); // XXX
  if(w > 550.0)
    magenta = 0.9*smoothstep(595.0, 645.0, w);
  // float yellow  = 0.92*smoothstep(492.0, 542.0, w) + 0.2*(1.0-smoothstep(370, 390, w));
  float yellow  = 0.92*smoothstep(475.0, 525.0, w) + 0.2*(1.0-smoothstep(370, 390, w)); // XXX
  return vec3(cyan, magenta, yellow);
}

vec2 hash(in ivec2 p)  // this hash is not production ready, please
{                        // replace this by something better
  ivec2 n = p.x*ivec2(3,37) + p.y*ivec2(311,113); // 2D -> 1D
  // 1D hash by Hugo Elias
  n = (n << 13) ^ n;
  n = n * (n * n * 15731 + 789221) + 1376312589;
  return -1.0+2.0*vec2( n & ivec2(0x0fffffff))/float(0x0fffffff);
}

// return gradient noise (in x) and its derivatives (in yz)
float noise(in vec2 p)
{
    ivec2 i = ivec2(floor( p ));
    vec2 f = fract( p );

    // quintic interpolation
    vec2 u = f*f*f*(f*(f*6.0-15.0)+10.0);
    vec2 du = 30.0*f*f*(f*(f-2.0)+1.0);

    vec2 ga = hash( i + ivec2(0,0) );
    vec2 gb = hash( i + ivec2(1,0) );
    vec2 gc = hash( i + ivec2(0,1) );
    vec2 gd = hash( i + ivec2(1,1) );

    float va = dot( ga, f - vec2(0.0,0.0) );
    float vb = dot( gb, f - vec2(1.0,0.0) );
    float vc = dot( gc, f - vec2(0.0,1.0) );
    float vd = dot( gd, f - vec2(1.0,1.0) );

    return va + u.x*(vb-va) + u.y*(vc-va) + u.x*u.y*(va-vb-vc+vd);   // value
                 // ga + u.x*(gb-ga) + u.y*(gc-ga) + u.x*u.y*(ga-gb-gc+gd) +  // derivatives
                 // du * (u.yx*(va-vb-vc+vd) + vec2(vb,vc) - va));
}

// separate density into three layers of grains (coarse, mid, fine) that sum up
// to the max density. lower density values are taken mostly by coarse grains.
// use perlin gradient noise / correlated to simulate the three layers (represents non-uniformity of grain counts/pixel)
// simulate a poisson distribution x3: are the grains turned?
// expectation should be =density, i.e. n*p + n*p + n*p = density for the three layers
vec3 add_grain(ivec2 ipos, vec3 density)
{
  int n_grains_per_pixel = 1000; // from artic's python. starts to look very good!
  int grain_non_uniformity = int((1.0-params.grain_uniformity)*n_grains_per_pixel);
  float grain_size = params.grain_size;
  float density_max = 3.3;
  vec3 res = vec3(0.0);
  uint seed = 123456789*ipos.x + 1333337*ipos.y;
  vec3 particle_scale_col = vec3(0.8, 1.0, 2.0);
  vec3 particle_scale_lay = vec3(2.5, 1.0, 0.5);
  for(int col=0;col<3;col++)
  {
    float np = density[col] / density_max; // expectation of normalised number of developed grains
    vec3 doff = vec3(0.10, 0.20, 0.7);
    for(int layer=0;layer<3;layer++)
    { // from coarse to fine layers
      float npl = min(doff[layer], np);
      np -= npl;
      float c = 1.0/(particle_scale_col[col] * particle_scale_lay[layer]);//pow(2, layer);
      vec2 tc = c/grain_size * 0.3*vec2(ipos + 1000*col);
      // int r = int(grain_non_uniformity*noise(tc));
      int r = int(grain_non_uniformity*c*noise(tc));
      int n = int(n_grains_per_pixel*c);
      float p = npl;// / float(n);
      // if(layer!=2) res[col] += density_max * npl; else
      // uint seed = n;//n_grains_per_pixel;
      // res[col] += density_max * (n+r)*p/float(n); // this is using the expected value of developed grains directly
      // res[col] += density_max * poisson(seed, (n+r)*p)/float(n); // simulates poisson, too bright
      // now simulate whether these grains actually turn:
      res[col] += density_max * binom(seed, (n+r), p)/float(n); // looks broken too
    }
  }
  return res;

#if 0
  // statistically sample whether grains are turned or not.
  // TODO the grains need inter/sub pixel stability, use simplex noise!
  // TODO poisson something is too slow above
  // TODO do it in the three layers, use the data we have in img_density_layers
  float grain_uniformity = 1.0;
  vec3 saturation = 1.0 - probability_of_development*grain_uniformity*(1.0-1e-6);
  float n_particles_per_pixel = 25.0;
  ivec3 ps = ivec3(
      poisson(seed, (n_particles_per_pixel / saturation).x),
      poisson(seed, (n_particles_per_pixel / saturation).y),
      poisson(seed, (n_particles_per_pixel / saturation).z));
  vec3 grain = vec3(
      binom(seed, ps.x, probability_of_development.x),
      binom(seed, ps.y, probability_of_development.y),
      binom(seed, ps.z, probability_of_development.z));
  grain *= saturation * density_max / n_particles_per_pixel;
  return grain;
#endif
}

const int s_sensitivity = 0;
const int s_dye_density = 1;
const int s_density_curve = 2;
float get_tcy(int type, int stock)
{
  const float s = textureSize(img_filmsim, 0).y;
  const int off = type;
  return (stock * 3 + off + 0.5)/s;
}

void main()
{
  ivec2 ipos = ivec2(gl_GlobalInvocationID);
  if(any(greaterThanEqual(ipos, imageSize(img_out)))) return;

  vec3 rgb = texelFetch(img_in, ipos, 0).rgb;

  // TODO: make parameters:
  int film  = params.film;
  int paper = 7 + params.paper;
  float dye_density_min_factor_film  = 1.0;
  float dye_density_min_factor_paper = 0.4;
  float gamma_factor_film  = params.gamma_film;
  float gamma_factor_paper = params.gamma_paper;
  float log_exp_min = -4.0;
  float log_exp_max =  4.0;
  // uint seed = /*19937 * global.frame + */133700000 * ipos.x + ipos.y * 70007;

  vec3 log_raw;
  { // film exposure in camera and chemical development
    vec4 coeff = fetch_coeff(rgb);
    vec2 tc = vec2(0, get_tcy(s_sensitivity, film));
    vec3 raw = vec3(0.0);
    for(int l=0;l<=40;l++)
    // for(int l=0;l<40;l++)
    {
      float lambda = 380 + l*10;
      // float lambda = 380 + l*10 + mrand(seed)*10;
      float val = sigmoid_eval(coeff, lambda);
      // not sure if needed: cuts off wavelength ranges that the spectral
      // upsampling doesn't care about since it is outside the XYZ support
      float env = envelope(lambda);
      tc.x = (l+0.5)/256.0;
      vec3 log_sensitivity = texture(img_filmsim, tc).rgb;
      vec3 sensitivity = pow(vec3(10.0), log_sensitivity);
      sensitivity = mix(sensitivity, vec3(0.0), isnan(sensitivity));
      raw += sensitivity * val * env / 40.0; // XXX uh something number of bins * 10nm or something
    }
    // TODO: the following two require to split the kernel and blur the intermediates:
      // TODO gaussian blur / lens radius
      // TODO apply halation
    log_raw = params.ev_film * (log(2.0)/log(10.0)) + log(raw+1e-10) * (1.0/log(10.0));
    // rgb = pow(vec3(10.0), log_raw);
    // imageStore(img_out, ipos, vec4(rgb, 1));
    // return;
    // rgb = texture(img_filmsim, vec2(0.5, get_tcy(0,0))).rgb;
  }

  vec3 density_cmy;
  { // develop film
    vec2 tc = vec2(0.0, get_tcy(s_density_curve, film));
    vec3 tcx = clamp((gamma_factor_film*log_raw - log_exp_min)/(log_exp_max-log_exp_min), vec3(0.0), vec3(1.0));
    density_cmy.r = texture(img_filmsim, vec2(tcx.r, tc.y)).r;
    density_cmy.g = texture(img_filmsim, vec2(tcx.g, tc.y)).g;
    density_cmy.b = texture(img_filmsim, vec2(tcx.b, tc.y)).b;
    density_cmy = mix(density_cmy, vec3(0.0), isnan(density_cmy));
    // TODO: this requires a kernel split/gauss blur.
    // computes new log_law and re-does the density-from-log-raw lut
#if 0
    // compute layer coupling 3x3 matrix by diffusion:
    M = np.eye(3) // 3x3 identity matrix
    amount_rgb=[0.7,0.7,0.5], layer_diffusion=1)
    // input, sigma, constant=0 border handling, only blur y axis
    M_diffused = gaussian_filter(M, layer_diffusion, mode='constant', cval=0, axes=1)
    M_diffused /= np.sum(M_diffused, axis=1)[:, None] // normalise the columns again
    // M = vec3(2, 1, 0,  1, 2, 1,  0, 1, 2) / 3;
    M = M_diffused *np.array(amount_rgb)[:, None] // multiply this to the uhm columns too?
    // compute density curves before dir couplers:
    d_max = np.nanmax(density_curves, axis=0) // = 3.3 i think
    dc_norm = density_curves/d_max // 3d log-exp density curves, normalised
    dc_norm_shift = dc_norm + high_exposure_couplers_shift*dc_norm**2 // defaults to +zero
    // correct the curves by (optionally shifting and) multiplying the matrix
    couplers_amount_curves = contract('jk, km->jm', dc_norm_shift, dir_couplers_matrix)
    // log_exposure is the image (log_raw from above)
    x0 = log_exposure[:,None] - couplers_amount_curves
    // compute new density curves by evaluating density curves shifted to (x0,dc) locations at log_raw
    // XXX wtf does this mean. awful lot of re-interpolation, i think this should be simpler:
    density_curves_corrected = np.zeros_like(density_curves)
    for i in np.arange(3):
        density_curves_corrected[:,i] = np.interp(log_exposure, x0[:,i], density_curves[:,i])
    return density_curves_corrected
    // now compute corrected log_raw exposure values:
    norm_density = density_cmy/density_max // 3d
    norm_density += high_exposure_couplers_shift*norm_density**2 // defaults to zero
    log_raw_correction = contract('ijk, km->ijm', norm_density, dir_couplers_matrix)// matrix M from above
    // i suppose this is potentially a larger support gaussian and definitely needs a kernel split
    // XXX we're linear filtering the log values here. does that make sense?
    log_raw_correction = gaussian_filter(log_raw_correction, (diffusion_size_pixel, diffusion_size_pixel, 0), truncate=7)
    log_raw_corrected = log_raw - log_raw_correction
    return log_raw_corrected // TODO get this as input from blur kernel
    // TODO: now re-run the density curve interpolation with new log_raw and new density_curves
#endif
    // TODO do this in commit_params on cpu side, there we know the actual scale factor:
    // hack to not have grain in preview images/thumbnails:
    if(params.grain > 0 && imageSize(img_out).x > 400) density_cmy = add_grain(ipos, density_cmy);
    rgb = density_cmy;
  }

  if(params.enlarger > 0)
  {

  { // enlarger: expose film to print paper
    vec3 raw = vec3(0.0);
    // vec3 thungsten = vec3(1.0985, 1.0000, 0.3558); // 2856K
    // vec4 coeff_l = fetch_coeff(XYZ_to_rec2020(thungsten));
    // sigmoidal transmission filters for cmy:
    // vec4 coeff_c = fetch_coeff(vec3(0.153, 1.0, 0.5));
    // vec4 coeff_m = fetch_coeff(vec3(1.0, -0.1, 2.0));
    // vec4 coeff_y = fetch_coeff(vec3(1.0, 0.3, 0.0));
    // coeff_c.w = coeff_m.w = coeff_y.w = 1.0; // we want reflectance/transmittance limited to 1.0 here
    vec3 neutral = vec3(params.filter_c, params.filter_m + 0.1*params.tune_m, params.filter_y + 0.1*params.tune_y);
    // XXX TODO clamp neutral to physical range? the fitting finds negative values for some paper/film combinations..
    for(int l=0;l<=40;l++)
    {
      float lambda = 380.0 + l*10.0;
      vec2 tc = vec2(0.0, get_tcy(s_sensitivity, paper));
      tc.x = (l+0.5) / 256.0;
      vec3 log_sensitivity = texture(img_filmsim, tc).rgb;
      vec3 sensitivity = pow(vec3(10.0), log_sensitivity);
      sensitivity = mix(sensitivity, vec3(0.0), isnan(sensitivity));

      tc = vec2(0.0, get_tcy(s_dye_density, film));
      tc.x = (l+0.5) / 256.0;
      vec4 dye_density = texture(img_filmsim, tc);
      dye_density = mix(dye_density, vec4(0.0), isnan(dye_density));
      float density_spectral = dot(vec3(1), density_cmy * dye_density.xyz);
      density_spectral += dye_density.w * dye_density_min_factor_film;

      // float illuminant = 0.001*colour_blackbody(vec4(lambda), 3200.0).x;
      // float illuminant = 0.03*colour_blackbody(vec4(lambda), 2200.0).x;
      float illuminant = 0.002*colour_blackbody(vec4(lambda), 2856.0).x;
      // float illuminant = 5*sigmoid_eval(coeff_l, lambda);
#if 1 // pretty coarse manual fit to thorlabs filters:
      vec3 enlarger = 100.0*mix(
      // vec3 enlarger = mix(
          vec3(1.0),
          thorlabs_filters(lambda),
          neutral);
#else
      // lamp filters are transmittances 0..100%
      vec3 enlarger = 100.0*mix(
          vec3(1.0),
          vec3(sigmoid_eval(coeff_c, lambda), sigmoid_eval(coeff_m, lambda), sigmoid_eval(coeff_y, lambda)),
          neutral);
#endif

      float print_illuminant = enlarger.x*enlarger.y*enlarger.z * illuminant;
      float light = pow(10.0, -density_spectral) * print_illuminant;
      raw += sensitivity * light * pow(2.0, params.ev_paper); // TODO and normalise to mid gray factor
      // TODO and the same yet again for the preflash
    }
    log_raw = log(raw + 1e-10)*(1.0/log(10.0));
    rgb = raw;
  }

  { // develop print
    // interpolate log exposure to density again
    vec2 tc = vec2(0.0, get_tcy(s_density_curve, paper));
    vec3 tcx = clamp((gamma_factor_paper*log_raw - log_exp_min)/(log_exp_max-log_exp_min), vec3(0.0), vec3(1.0));
    density_cmy.r = texture(img_filmsim, vec2(tcx.r, tc.y)).r;
    density_cmy.g = texture(img_filmsim, vec2(tcx.g, tc.y)).g;
    density_cmy.b = texture(img_filmsim, vec2(tcx.b, tc.y)).b;
    rgb = density_cmy;
  }
  }

  { // scan
    // convert cmy density to spectral
    // absorption / dye density of developed film
    vec3 raw = vec3(0.0);
    vec3 d50 = vec3(0.9642, 1.0000, 0.8251); // 5002K
    vec4 coeff = fetch_coeff(d50);
    for(int l=0;l<=40;l++)
    {
      float lambda = 380 + l*10;
      vec4 dye_density = texture(img_filmsim, vec2((l+0.5) / 256.0,
            get_tcy(s_dye_density, params.enlarger > 0 ? paper : film)));
      dye_density = mix(dye_density, vec4(0.0), isnan(dye_density));
      float density_spectral = dot(vec3(1), density_cmy * dye_density.xyz);
      density_spectral += dye_density.w * dye_density_min_factor_paper;
      float scan_illuminant = 0.105*sigmoid_eval(coeff, lambda);
      float light = pow(10.0, -density_spectral) * scan_illuminant;
      vec3 cmf = cmf_1931(lambda); // 1931 2 deg std observer, approximate version
      raw += light * cmf;
    }

    rgb = XYZ_to_rec2020(raw);
    // TODO add glare
    // TODO _apply_blur_and_unsharp
  }
  imageStore(img_out, ipos, vec4(rgb, 1));
}
